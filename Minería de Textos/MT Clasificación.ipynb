{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de textos\n",
    "\n",
    "En esta clase, exploraremos datos de mensajes de texto y crearemos modelos para predecir si un mensaje es spam o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv(r'C:\\Users\\FLopezP\\Documents\\GitHub\\PCIC\\Minería de Textos\\Archivos\\spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1\n",
    "¿Qué porcentaje de los documentos en `spam_data` son spam?\n",
    "\n",
    "*Esta función debe devolver un valor flotante, el valor porcentual (es decir, $ ratio * 100 $).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_uno():\n",
    "    aux = spam_data['target'].value_counts()\n",
    "    return aux[1]/(aux[0]+aux[1])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_uno()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "Luego, ajuste un modelo de clasificación Naive Bayes multinomial. Calcule medidas de exactitud, presición, recall y f1-score usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver las cuatro medidas de evaluación como una lista con los 4 valores en el orden solicitado cada valor como flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def respuesta_dos():\n",
    "    scores=[]\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    \n",
    "    clf=MultinomialNB()\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    print(\"matriz Frecuencias:\",X_train_vectorized.shape, \"clases\",len(y_train))\n",
    "    \n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "    print(\"matriz Frecuencias Test:\",X_test_vectorized.shape, \"prediction\",len(predictions))\n",
    "    \n",
    "    scores.append(accuracy_score(y_test,predictions))\n",
    "    scores.append(precision_score(y_test,predictions))\n",
    "    scores.append(recall_score(y_test,predictions))\n",
    "    scores.append(f1_score(y_test,predictions))\n",
    "    \n",
    "    spam=predictions[predictions!= 0]\n",
    "    print('Predicciones Spam:',len(spam))\n",
    "    print('Porcentaje Spam:',len(spam)/len(predictions)*100)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(tn, fp, fn, tp)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matriz Frecuencias: (4179, 7354) clases 4179\n",
      "matriz Frecuencias Test: (1393, 7354) prediction 1393\n",
      "Predicciones Spam: 184\n",
      "Porcentaje Spam: 13.208901651112706\n",
      "[[1193    3]\n",
      " [  16  181]]\n",
      "1193 3 16 181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9863603732950467,\n",
       " 0.9836956521739131,\n",
       " 0.9187817258883249,\n",
       " 0.9501312335958005]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_dos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "Luego, ajuste un modelo de clasificación Naive Bayes multinomial con suavizado (smoothing) `alpha = 0.1`. Encuentre el área bajo la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def respuesta_tres():\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    clf=MultinomialNB(alpha=0.1)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9720812182741116"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_tres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **3**.\n",
    "\n",
    "Luego, ajuste un modelo de clasificador Naive Bayes multinomial con suavizado (smoothing) `alfa = 0.1` y calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def respuesta_cuatro():\n",
    "    vector = TfidfVectorizer(min_df = 3)\n",
    "    x_vector = vector.fit_transform(X_train)\n",
    "    NB=MultinomialNB(alpha=0.1)\n",
    "    NB.fit(x_vector, y_train)\n",
    "    x_text_vector = vector.transform(X_test)\n",
    "    pred = NB.predict(x_text_vector)\n",
    "    return roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416243654822335"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_cuatro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5\n",
    "\n",
    "¿Cuál es la longitud promedio de los documentos (número de caracteres) para los documentos spam y no spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (longitud promedio no spam, longitud promedio de spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_cinco():\n",
    "    \"\"\"\n",
    "    Regresa el promedio de de caracteres por clase de documentos\n",
    "    \"\"\"\n",
    "    no_spam = [spam_data[\"text\"][_] for _ in range(len(spam_data)) if spam_data[\"target\"].iloc[_] == 0]\n",
    "    yes_spam = [spam_data[\"text\"][_] for _ in range(len(spam_data)) if spam_data[\"target\"].iloc[_] == 1]\n",
    "    \n",
    "    no_avg = 0\n",
    "    yes_avg = 0\n",
    "    \n",
    "    for _ in no_spam:\n",
    "        no_avg += len(_)\n",
    "    \n",
    "    for _ in yes_spam:\n",
    "        yes_avg += len(_)\n",
    "    \n",
    "    return no_avg/len(no_spam), yes_avg/len(yes_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_cinco()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "**¿Qué onda con esta celda?**\n",
    "\n",
    "The following function has been provided to help you combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5**.\n",
    "\n",
    "Usando esta matriz de término de documento y una característica adicional, **la longitud del documento (número de caracteres)**, ajustar a un modelo de Clasificación de Vector de Soporte con regularización `C = 10000`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def respuesta_seis():\n",
    "    vector = TfidfVectorizer(min_df = 5)\n",
    "    x_vector = vector.fit_transform(X_train)\n",
    "    len_list = np.array([len(_) for _ in X_train]).reshape(-1,1) # Creamos este array para extender la matriz tfidf\n",
    "    x_vector_new = sparse.hstack((x_vector, len_list)).A # Usamos hstack para extender la matriz ya que está en formato csr_matrix\n",
    "    \n",
    "    svc = SVC(C = 10000)\n",
    "    svc.fit(x_vector_new, y_train)\n",
    "    \n",
    "    x_text_vector = vector.transform(X_test)\n",
    "    test_len_list = np.array([len(_) for _ in X_test]).reshape(-1,1) # Análogo para el test set\n",
    "    x_text_vector_new = sparse.hstack((x_text_vector, test_len_list)).A\n",
    "    pred = svc.predict(x_text_vector_new)\n",
    "    \n",
    "    return roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9661689557407943"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_seis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 7\n",
    "\n",
    "¿Cuál es el número promedio de dígitos por documento para los documentos no spam y spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # dígitos no es spam, promedio # dígitos spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_digits(lista):\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    digits = 0\n",
    "    for _ in lista:\n",
    "        aux = 0\n",
    "        for j in _:\n",
    "            if j in nums:\n",
    "                aux += 1\n",
    "        digits += aux\n",
    "\n",
    "    return digits/len(lista)\n",
    "\n",
    "    \n",
    "def respuesta_siete():\n",
    "    no_spam = [re.sub('[\\\\n\\-–¿?,.\":¡!]+', '', spam_data[\"text\"].iloc[_]) for _ in range(len(spam_data)) if spam_data[\"target\"].iloc[_] == 0]\n",
    "    yes_spam = [re.sub('[\\\\n\\-–¿?,.\":¡!]+', '', spam_data[\"text\"].iloc[_]) for _ in range(len(spam_data)) if spam_data[\"target\"].iloc[_] == 1]\n",
    "\n",
    "    a = get_digits(no_spam)\n",
    "    b = get_digits(yes_spam)\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2992746113989637, 15.759036144578314)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_siete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 8\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y usando **n-gramas de palabras n = 1 a n = 3** (unigramas, bigramas y trigramas).\n",
    "\n",
    "Usando esta matriz de término-documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* **cantidad de dígitos por documento**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def digit_count(value):\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    digits = 0\n",
    "    for _ in value:\n",
    "        if _ in nums:\n",
    "            digits += 1\n",
    "    return digits\n",
    "    \n",
    "def respuesta_ocho():\n",
    "    vector = TfidfVectorizer(min_df = 5, ngram_range = (1,3))\n",
    "    x_vector = vector.fit_transform(X_train)\n",
    "    len_list = np.array([len(_) for _ in X_train]).reshape(-1,1) # Creamos este array para extender la matriz tfidf\n",
    "    dig_list = np.array([digit_count(_) for _ in X_train]).reshape(-1,1)\n",
    "    x_vector_new = sparse.hstack((x_vector, len_list))\n",
    "    final_vector = sparse.hstack((x_vector_new, dig_list)) \n",
    "\n",
    "    lr = LogisticRegression(C = 100)\n",
    "    lr.fit(final_vector, y_train)\n",
    "\n",
    "    x_test_vector = vector.transform(X_test)\n",
    "    len_test = np.array([len(_) for _ in X_test]).reshape(-1,1)\n",
    "    dig_test = np.array([digit_count(_) for _ in X_test]).reshape(-1,1)\n",
    "    test_matrix = sparse.hstack((x_test_vector, len_test))\n",
    "    test_matrix = sparse.hstack((test_matrix, dig_test))\n",
    "\n",
    "    pred = lr.predict(test_matrix)\n",
    "    return roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9809793219360643"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_ocho()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 9\n",
    "\n",
    "¿Cuál es el número promedio de caracteres que no son palabras (cualquier cosa que no sea una letra, un dígito o un guión bajo) por documento para los documentos que no son spam y spam?\n",
    "\n",
    "*Sugerencia: utilice las clases de caracteres `\\ w` y` \\ W`*\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # caracteres que no son palabras, no spam, promedio de # caracteres que no son palabras, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_not_char(lista):\n",
    "    avg = 0\n",
    "    for _ in lista:\n",
    "        avg += len(_)\n",
    "    return avg/len(lista)\n",
    "    \n",
    "def pregunta_nueve():\n",
    "    no_spam = [re.sub('([\\w\\d_])', '', spam_data[\"text\"].iloc[_]).lower() for _ in range(len(spam_data)) if spam_data[\"target\"].iloc[_] == 0]\n",
    "    yes_spam = [re.sub('([\\w\\d_])', '', spam_data[\"text\"].iloc[_]).lower() for _ in range(len(spam_data)) if spam_data[\"target\"].iloc[_] == 1]\n",
    "    a = get_not_char(no_spam)\n",
    "    b = get_not_char(yes_spam)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.29181347150259, 29.041499330655956)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta_nueve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 10\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `CountVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y utilizando **caracteres n-grams desde n = 2 a n = 5.**\n",
    "\n",
    "Para decirle a `CountVectorizer` que use caracteres n-grams, pase en `analyzer = 'char_wb'` que crea caracteres n-gramas solo del texto dentro de los límites de las palabras. Esto debería hacer que el modelo sea más robusto a los errores ortográficos.\n",
    "\n",
    "Usando esta matriz término documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* cantidad de dígitos por documento\n",
    "* **cantidad de caracteres que no son palabras (cualquier cosa que no sea una letra, dígito o guión bajo).**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "También **encuentre los 10 coeficientes más pequeños y los 10 más grandes del modelo** y devuélvalos junto con el AUC en una tupla.\n",
    "\n",
    "La lista de los 10 coeficientes más pequeños debe ordenarse primero, la lista de los 10 coeficientes más grandes debe ordenarse primero.\n",
    "\n",
    "Las tres características que se agregaron a la matriz de términos del documento deben tener los siguientes nombres si aparecen en la lista de coeficientes:\n",
    "['longitud_doc', 'conteo_digito', 'caracteres_no_palabra']\n",
    "\n",
    "*Esta función debe devolver una tupla `(AUC como flotante, lista de coeficientes más pequeños, lista de coeficientes más grande)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_words(text):\n",
    "    new_text = re.sub('([\\w\\d_])', '', text).lower().split()\n",
    "    return len(new_text)\n",
    "    \n",
    "def respuesta_10():\n",
    "    vector = TfidfVectorizer(analyzer = 'char_wb', min_df = 5, ngram_range = (2,5))\n",
    "    x_vector = vector.fit_transform(X_train)\n",
    "    len_list = np.array([len(_) for _ in X_train]).reshape(-1,1) # Creamos este array para extender la matriz tfidf\n",
    "    dig_list = np.array([digit_count(_) for _ in X_train]).reshape(-1,1)\n",
    "    non_word_list = np.array([get_non_words(_) for _ in X_train]).reshape(-1,1)\n",
    "    x_vector_new = sparse.hstack((x_vector, len_list))\n",
    "    almost_new = sparse.hstack((x_vector_new, dig_list))\n",
    "    x_vector_final = sparse.hstack((almost_new, non_word_list))\n",
    "    \n",
    "    lr = LogisticRegression(C = 100)\n",
    "    lr.fit(x_vector_final, y_train)\n",
    "    \n",
    "    test_len = np.array([len(_) for _ in X_test]).reshape(-1,1)\n",
    "    test_dig = np.array([digit_count(_) for _ in X_test]).reshape(-1,1)\n",
    "    test_non_word = np.array([get_non_words(_) for _ in X_test]).reshape(-1,1)\n",
    "    \n",
    "    x_test_vector = vector.transform(X_test)\n",
    "    x_test_vector = sparse.hstack((x_test_vector, test_len))\n",
    "    x_test_vector = sparse.hstack((x_test_vector, test_dig))\n",
    "    x_test_vector = sparse.hstack((x_test_vector, test_non_word))\n",
    "    \n",
    "    pred = lr.predict(x_test_vector)\n",
    "    weights = lr.decision_function(x_vector_final)\n",
    "    weights.sort()\n",
    "    return roc_auc_score(y_test, pred), weights[:10], weights[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9780231906694056,\n",
       " array([-20.80264029, -18.23397765, -18.20244179, -18.06020586,\n",
       "        -18.05843282, -17.39323182, -17.31403965, -17.19804971,\n",
       "        -17.11159151, -16.58443461]),\n",
       " array([24.76197841, 24.77268886, 25.09554606, 25.11621127, 25.13226832,\n",
       "        25.97749762, 26.95930595, 28.82688043, 29.35643523, 29.35643523]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
