{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c647374-d884-4c31-93ae-9d9208bb4312",
   "metadata": {},
   "source": [
    "# Ej1: Clasificación de rostros: ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a965f19-9ce6-42df-beb3-cd4bbdda7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabrones ya no quiero hacer tarea me lleva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df4505-54b1-4c87-aa97-5cda438fe007",
   "metadata": {},
   "source": [
    "# Ej2: Reconocimiento de comandos de voz\n",
    "\n",
    "La importación de datos se hace basándonos en el notebook oficial del curso [(Github de Gibrán)](https://github.com/gibranfp/CursoAprendizajeProfundo/blob/2026-1/notebooks/1d_procesamiento_audio.ipynb). La red ya es una implementación personal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f458f-403c-4c0e-b70c-3720cd8bf437",
   "metadata": {},
   "source": [
    "## 2.1 Datos, Dataset, y Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fdb8880-c5d3-42c5-853e-8bf8c67ce38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import librosa\n",
    "import IPython as ip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from tqdm.auto import trange\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c3bcfb-43d7-4830-8815-f83a3e42964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_ds = SPEECHCOMMANDS(r'C:\\Users\\FLopezP\\Desktop\\PCIC\\Tercer Semestre\\Datasets', download = False) # download = True solo si no lo predescargaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51211df5-82ff-41dc-8d3a-bd0e9d62983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backward': 0, 'bed': 1, 'bird': 2, 'cat': 3, 'dog': 4, 'down': 5, 'eight': 6, 'five': 7, 'follow': 8, 'forward': 9, 'four': 10, 'go': 11, 'happy': 12, 'house': 13, 'learn': 14, 'left': 15, 'marvin': 16, 'nine': 17, 'no': 18, 'off': 19, 'on': 20, 'one': 21, 'right': 22, 'seven': 23, 'sheila': 24, 'six': 25, 'stop': 26, 'three': 27, 'tree': 28, 'two': 29, 'up': 30, 'visual': 31, 'wow': 32, 'yes': 33, 'zero': 34}\n"
     ]
    }
   ],
   "source": [
    "CLASSES = (\n",
    "    'backward', 'bed', 'bird', 'cat', 'dog',\n",
    "    'down', 'eight', 'five', 'follow', 'forward',\n",
    "    'four', 'go', 'happy', 'house', 'learn',\n",
    "    'left', 'marvin', 'nine', 'no', 'off',\n",
    "    'on', 'one', 'right', 'seven', 'sheila',\n",
    "    'six', 'stop', 'three', 'tree', 'two',\n",
    "    'up', 'visual', 'wow', 'yes', 'zero'\n",
    ")\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "CLASS_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "print(CLASS_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7ddfbd-af89-4c78-a628-bcb9aaa6f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros. (Extraídos del notebook)\n",
    "BATCH_SIZE = 32\n",
    "SECS = 1\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = N_FFT // 2\n",
    "\n",
    "class MySpeechCommands(SPEECHCOMMANDS):\n",
    "    \"\"\"\n",
    "    Clase para crear el dataloader. (Extraída del notebook)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "               root, \n",
    "               download = False, \n",
    "               subset = None,\n",
    "               waveform_tsfm = None, \n",
    "               label_tsfm = None):\n",
    "        super().__init__(root = root, download = download, subset = subset)\n",
    "        self.waveform_tsfm = waveform_tsfm\n",
    "        self.label_tsfm = label_tsfm\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        waveform, sample_rate, label, *_ = super().__getitem__(i)\n",
    "    \n",
    "        if self.waveform_tsfm:\n",
    "            x = self.waveform_tsfm(waveform)\n",
    "        \n",
    "        if self.label_tsfm:\n",
    "            y = self.label_tsfm(label)\n",
    "          \n",
    "        return x, y, label, sample_rate\n",
    "      \n",
    "class WaveformPadTruncate(nn.Module):\n",
    "    \"\"\"\n",
    "    Clase para crear el dataloader. (Extraída del notebook)\n",
    "    \"\"\"\n",
    "    def __init__(self, secs = SECS, sample_rate = SAMPLE_RATE):\n",
    "        super().__init__()\n",
    "        self.samples = secs * sample_rate\n",
    "    \n",
    "    def forward(self, waveform):\n",
    "        samples = waveform.shape[1]\n",
    "    \n",
    "        if samples < self.samples:\n",
    "            difference = self.samples - samples\n",
    "            padding = th.zeros(1, difference)\n",
    "            waveform = th.cat([waveform, padding], 1)\n",
    "        \n",
    "        elif samples > self.samples:\n",
    "            start = random.randint(0, waveform.shape[1] - self.samples)\n",
    "            # Devuelve un nuevo tensor que es una versión reducida del tensor de entrada.\n",
    "            waveform = waveform.narrow(1, start, self.samples) # (dimension, start, length)\n",
    "            \n",
    "        return waveform\n",
    "\n",
    "def label2index(label):\n",
    "    return CLASS_IDX[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfcc41e9-fe47-45e9-96a6-05c9cf2385bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del dataset y dataloader.\n",
    "ds = MySpeechCommands(\n",
    "  root = r'C:\\Users\\FLopezP\\Desktop\\PCIC\\Tercer Semestre\\Datasets',\n",
    "  waveform_tsfm = WaveformPadTruncate(),\n",
    "  label_tsfm = label2index,\n",
    ")\n",
    "\n",
    "# creamos un DataLoader\n",
    "dl = DataLoader(\n",
    "  ds,\n",
    "  batch_size = BATCH_SIZE,\n",
    "  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602a39c7-d374-40d2-b8ea-36f20d858e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = torch.Size([32, 1, 16000]) dtype = torch.float32\n",
      "y shape = torch.Size([32]) dtype = torch.int64\n",
      "label = ('one', 'seven', 'eight', 'go', 'no', 'wow', 'backward', 'visual', 'up', 'no', 'nine', 'one', 'down', 'four', 'up', 'marvin', 'zero', 'backward', 'bird', 'off', 'seven', 'happy', 'visual', 'no', 'go', 'no', 'backward', 'tree', 'two', 'eight', 'left', 'learn') type = <class 'tuple'>\n",
      "sr tensor shape = torch.Size([32]) type = <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x, y, label, sr = next(iter(dl))\n",
    "print(f'x shape = {x.shape} dtype = {x.dtype}')\n",
    "print(f'y shape = {y.shape} dtype = {y.dtype}')\n",
    "print(f'label = {label} type = {type(label)}')\n",
    "print(f'sr tensor shape = {sr.shape} type = {type(sr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bb536-cc04-467f-917d-e11b3c99fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Obs: Se opta por usar nn.TransformerEncoderLayer(*params), en vez de nn.Transformer(*params) por cuestiones de capacidades de cómputo.\n",
    "# Para que el bloque transformer sea capaz de trabajar se necesita que el las características de la entrada sean iguales al parámetro \"d_model\".\n",
    "# En nuestro caso tenemos que las características corresponden a 16000. Si uso el parámetro d_model = 16000 mi compu llega al borde de la muerte.\n",
    "\n",
    "#trans = nn.Transformer(d_model = 16000, nhead = 8, num_encoder_layers=6)\n",
    "aux = nn.TransformerEncoderLayer(d_model = 16000, nhead = 8)\n",
    "enc_aux = nn.TransformerEncoder(aux, num_layers = 2)\n",
    "output = enc_aux(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdfb179b-f879-48a1-ae96-60d6ac8109d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3086,  0.2007,  0.1158,  ..., -0.1314,  0.6250,  0.4532]],\n",
       "\n",
       "        [[ 0.3681,  0.5013,  0.4103,  ..., -0.0792,  0.3575, -0.7236]],\n",
       "\n",
       "        [[ 0.1415, -0.1647, -0.1307,  ..., -0.3036, -0.6703, -0.0168]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4008, -0.1394,  0.0151,  ..., -0.5045,  0.4041, -0.1784]],\n",
       "\n",
       "        [[ 0.6218,  0.7743, -0.6431,  ..., -1.4349,  0.4689, -0.2668]],\n",
       "\n",
       "        [[-0.8993,  0.4858, -0.8273,  ..., -0.7159, -0.7614, -1.1735]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dec = nn.TransformerDecoderLayer(d_model = 16000, nhead = 4)\n",
    "decode = nn.TransformerDecoder(dec, num_layers = 2)\n",
    "decode(output, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fea58-0667-442e-ba76-9016fa789754",
   "metadata": {},
   "source": [
    "## 2.2 Red Neuronal basada en Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "390cfd70-dd2b-4bc4-b523-0fd0e1dd7487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RedNeuronal(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (enc_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=16000, out_features=16000, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=16000, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=16000, bias=True)\n",
      "    (norm1): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dec_layer): TransformerDecoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=16000, out_features=16000, bias=True)\n",
      "    )\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=16000, out_features=16000, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=16000, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=16000, bias=True)\n",
      "    (norm1): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (proc1): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16000, out_features=16000, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16000, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=16000, bias=True)\n",
      "        (norm1): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proc2): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16000, out_features=16000, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16000, out_features=16000, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16000, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=16000, bias=True)\n",
      "        (norm1): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((16000,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classification): Linear(in_features=16000, out_features=35, bias=True)\n",
      ")\n",
      "CPU times: total: 1min 31s\n",
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class RedNeuronal(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        La red consta de un bloque transformer con 2 bloques de: 4 Encoders y 2 Decoders. (Se usan estas cantidades debido a restricciones de cómputo)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enc_layer = nn.TransformerEncoderLayer(d_model = 16000, nhead = 4)\n",
    "        self.dec_layer = nn.TransformerDecoderLayer(d_model = 16000, nhead = 2)\n",
    "        self.proc1 = nn.TransformerEncoder(self.enc_layer, num_layers = 1)\n",
    "        self.proc2 = nn.TransformerDecoder(self.dec_layer, num_layers = 1)\n",
    "        self.classification = nn.Linear(16000, NUM_CLASSES)\n",
    "        #self.softmax = nn.Softmax(dim = 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc = self.proc1(x)\n",
    "        dec = self.proc2(enc,enc)\n",
    "        logits = self.classification(dec)\n",
    "        return logits\n",
    "\n",
    "red_cabrona = RedNeuronal()\n",
    "print(red_cabrona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a53de9cd-f8ca-4f67-83d3-41423f40c9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=16000, out_features=16000, bias=True)\n",
      ") MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=16000, out_features=16000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Para encontrar los mappings hay que llegar a esto.\n",
    "print(red_SEXO.enc_layer.self_attn, red_SEXO.proc2.layers[1].self_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7030204-d6b8-4d49-8f0a-5c0de1fb7cb8",
   "metadata": {},
   "source": [
    "# Ej3: Generación de rostros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66228142-e0c1-438c-ab31-a6ac3dc503a8",
   "metadata": {},
   "source": [
    "# Extra: Generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a822cd-3c20-49c4-928f-f0bfff3a6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ayuda dios por favor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
