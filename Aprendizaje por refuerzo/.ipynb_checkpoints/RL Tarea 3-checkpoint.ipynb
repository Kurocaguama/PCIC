{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9e9cdc-5474-45a4-906d-4c644fc26e23",
   "metadata": {},
   "source": [
    "**Comentarios**\n",
    "\n",
    "1. No se implementó ni NES ni GA.\n",
    "2. Algo de mi implementación siento que no me permite graficar de buena forma los resultados. Ayuda por favor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecc7ad5-956d-41ad-992f-6d3a5cb75c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math, random, matplotlib, warnings, torch, ale_py, stable_baselines3, collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import typing as tt\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2671747b-ae21-4288-a425-740f1d30bde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donkey Kong. Observation Space: (210, 160, 3). Action Space: 18\n",
      "Breakout. Obs Space (210, 160, 3). Action Space: 4\n"
     ]
    }
   ],
   "source": [
    "donkey_kong = gym.make(\"ALE/DonkeyKong-v5\")\n",
    "breakout = gym.make(\"ALE/Breakout-v5\")\n",
    "\n",
    "print(f\"Donkey Kong. Observation Space: {donkey_kong.observation_space.shape}. Action Space: {donkey_kong.action_space.n}\")\n",
    "print(f\"Breakout. Obs Space {breakout.observation_space.shape}. Action Space: {breakout.action_space.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca0a7d-c879-40bb-9fb0-64843e3d6c5f",
   "metadata": {},
   "source": [
    "## Graficar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aab2653-7899-4029-aec3-65693393486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(reward_recs, title):\n",
    "    avg_reward = []\n",
    "    for _ in range(len(reward_recs)):\n",
    "        avg = np.empty(shape=(1,), dtype =int)\n",
    "        if _ < 20:\n",
    "            avg = reward_recs[:_+1]\n",
    "        else:\n",
    "            avg = reward_recs[_-19:_+1]\n",
    "        avg_reward.append(np.average(avg))\n",
    "    plt.plot(reward_recs)\n",
    "    plt.plot(avg_reward)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Recompensa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9f5b2-c8c4-461b-883a-65ddaa76d240",
   "metadata": {},
   "source": [
    "## Clases para la implementación\n",
    "\n",
    "Se tiene una implementación independiente para cada algoritmo en cuestión. Si bien son implementaciones bastante similares, cada clase presenta diferencias de implementación en función del algortimo para el que se estén planteando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a33dad-eda5-4dc8-8d1a-2d8db2c37c1e",
   "metadata": {},
   "source": [
    "### Políticas (Redes neuronales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d6d882f-bda0-4702-b2ee-05770cc4f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Política para PPO\n",
    "class Conv_Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    Esta clase implementa todo lo necesario para que el agente interactúe en el ambiente.\n",
    "    El entrenamiento de las redes correspondientes (self.actor, self.value) se realiza \n",
    "    durante la implementación misma del algoritmo.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: tt.Tuple[int, ...],\n",
    "        n_actions: int\n",
    "    ):\n",
    "        super(Conv_Policy, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, kernel_size = 3, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 1, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(0)\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            size = self.conv(torch.zeros(1, *input_shape)).flatten().size()[-1]\n",
    "            \n",
    "        self.actor = nn.Sequential(\n",
    "            self.conv,\n",
    "            nn.Linear(size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, kernel_size = 3, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 1, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(0)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            self.conv2,\n",
    "            nn.Linear(size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Nos permite encontrar una acción siguiendo nuestra política.\n",
    "        \"\"\"\n",
    "        \n",
    "        action_proba = self.actor(state)\n",
    "        distribution = Categorical(action_proba)\n",
    "\n",
    "        action = distribution.sample()\n",
    "        action_log_prob = distribution.log_prob(action)\n",
    "        state_value = self.critic(state) \n",
    "\n",
    "        return action.detach(), action_log_prob.detach(), state_value.detach()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.actor(state)\n",
    "        distribution = Categorical(action_probs)\n",
    "        action_log_prob = distribution.log_prob(action)\n",
    "        distribution_entropy = distribution.entropy()\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_log_prob, state_values, distribution_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662dec8f-6dc2-4b86-bb29-59cbe1d6d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Política para DDQN\n",
    "class Conv_PolicyDDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Se modifica la clase de Conv_Policy. Ya no se tiene una red de actor y otra de crítico.\n",
    "    También se simplifica el proceso de elegir una acción en función de la red. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: tt.Tuple[int, ...],\n",
    "        n_actions: int\n",
    "    ):\n",
    "        super(Conv_PolicyDDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, kernel_size = 3, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 1, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(0)\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            size = self.conv(torch.zeros(1, *input_shape)).flatten().size()[-1]\n",
    "            \n",
    "        self.Qnetwork = nn.Sequential(\n",
    "            self.conv,\n",
    "            nn.Linear(size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "            nn.Softmax(dim=-1) \n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Nos permite encontrar una acción siguiendo nuestra política.\n",
    "        Como buscamos muestrear podemos no considerar state_value, ni log_prob.\n",
    "        \"\"\"\n",
    "        action_proba = self.Qnetwork(state)\n",
    "        distribution = Categorical(action_proba)\n",
    "\n",
    "        action = distribution.sample()\n",
    "        return action.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef80305-f0fb-4b54-843f-bb44090fc718",
   "metadata": {},
   "source": [
    "### Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed28e95-1d1e-4954-8778-d686f09f4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffers:\n",
    "    \"\"\"\n",
    "    Esta clase nos permite guardar información sobre la ejecución de nuestro algoritmo.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class Buffers_DDQN:\n",
    "    \"\"\"\n",
    "    Esta clase nos permite guardar información sobre la ejecución de nuestro algoritmo.\n",
    "    A diferencia de PPO, no necesitamos los valores del estado ni las logprobs.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "        self.is_terminal = []\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminal[:]\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Vamos a muestrear a partir del buffer cargado dentro de una interacción con el ambiente.\n",
    "        \"\"\"\n",
    "        # Extracción aleatoria a partir de la muestra\n",
    "        max_range = len(self.states)\n",
    "        random_index = random.randint(0, max_range-2)\n",
    "        action = self.actions[random_index] \n",
    "        state = self.states[random_index]\n",
    "        next_state = self.states[random_index+1]\n",
    "        reward = [self.rewards[random_index]]\n",
    "        term = [self.is_terminal[random_index]]\n",
    "\n",
    "        # Formateo de datos para procesamiento posterior.\n",
    "        # Odio a pytorch\n",
    "        if type(action) == int:\n",
    "            action = torch.tensor(action, dtype = torch.float32)\n",
    "        else:\n",
    "            action = action.to(torch.float32).to(dev)\n",
    "        state = torch.from_numpy(state).to(dev)\n",
    "        state = state.to(torch.float32)\n",
    "\n",
    "        next_state = torch.from_numpy(next_state).to(dev)\n",
    "        next_state = next_state.to(torch.float32)\n",
    "\n",
    "        reward = torch.Tensor(reward).to(dev)\n",
    "        term = torch.Tensor(term).to(dev)\n",
    "            \n",
    "        return state, action, reward, next_state, term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0fd8f-3269-425e-80c7-efccc930454d",
   "metadata": {},
   "source": [
    "### Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1ef33-f963-4645-b232-d0ff9fa8bb1b",
   "metadata": {},
   "source": [
    "**PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f44cb93-7044-4c58-be1b-ddb87a8b0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_Self:\n",
    "    \"\"\"\n",
    "    La clase PPO contiene la información relevante para la implementación del algoritmo mismo.\n",
    "    El entrenamiento se hace mediante una función externa que trabaja sobre una instancia de esta clase.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr_actor,\n",
    "        lr_critic,\n",
    "        gamma,\n",
    "        epochs,\n",
    "        clip,\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.clip = clip\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Definimos políticas, \n",
    "        \n",
    "        self.buffer = Buffers()\n",
    "        self.policy = Conv_Policy(state_dim, action_dim).to(dev)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "            ])\n",
    "        self.policy_prev = Conv_Policy(state_dim, action_dim).to(dev)\n",
    "        self.Mseloss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Elección de acción en función de la política previa.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(dev)\n",
    "            action, action_log_prob, state_val = self.policy_prev.act(state)\n",
    "            \n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.logprobs.append(action_log_prob)\n",
    "        self.buffer.state_values.append(state_val)\n",
    "        return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Actualización de pesos, esta función se llama durante el entrenamiento.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        disc_reward = 0\n",
    "        for reward, is_term in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_term:\n",
    "                disc_reward = 0\n",
    "            disc_reward = reward + (self.gamma * disc_reward)\n",
    "            rewards.insert(0, disc_reward)\n",
    "\n",
    "        rewards = torch.tensor(rewards, dtype = torch.float32).to(dev)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
    "\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(dev)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(dev)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(dev)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(dev)\n",
    "\n",
    "        advantage = rewards.detach() - old_state_values.detach()\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            for state in old_states:\n",
    "                logprobs, state_values, dist_entropy = self.policy.evaluate(state, old_actions)\n",
    "                state_values = torch.squeeze(state_values)\n",
    "                ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "                # CLIP\n",
    "                surrogate1 = ratios*advantage\n",
    "                surrogate2 = torch.clamp(ratios, 1-self.clip, 1+self.clip) * advantage\n",
    "    \n",
    "                loss = -torch.min(surrogate1, surrogate2) + 0.5*self.Mseloss(state_values, rewards) - 0.01*dist_entropy\n",
    "\n",
    "                # Actualización de pesos\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        self.policy_prev.load_state_dict(self.policy.state_dict())\n",
    "        self.buffer.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce51b8a0-2964-442f-b791-ff3ef7794a3c",
   "metadata": {},
   "source": [
    "**DDQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5da802f5-0cb0-4553-8661-bb9469400913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_Self:\n",
    "    \"\"\"\n",
    "    Esta implementación se hace sobre la segunda versión de DDQN propuesta por Hasselt en el 2015.\n",
    "    En vez de trabajar con dos políticas y modificarlas de manera aleatoria, se tiene una política base\n",
    "    Y una política objetivo que se busca aproximar. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr,\n",
    "        gamma,\n",
    "        tau,\n",
    "        epochs\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.epochs = epochs\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.action_dim = action_dim #hmmmmmmm\n",
    "        \n",
    "        self.buffer = Buffers_DDQN()\n",
    "        self.model = Conv_PolicyDDQN(state_dim, action_dim).to(dev)\n",
    "        self.target = Conv_PolicyDDQN(state_dim, action_dim).to(dev)\n",
    "\n",
    "        for target_net, net in zip(self.model.parameters(), self.target.parameters()):\n",
    "            target_net.data.copy_(net)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = lr)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        explore = 0.1\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(dev)\n",
    "            action = self.model.act(state) \n",
    "            if np.random.rand() < explore:\n",
    "                action = random.randrange(self.action_dim)\n",
    "            return action\n",
    "\n",
    "    def q_loss(self):\n",
    "        state, action, reward, next_state, term = self.buffer.sample()\n",
    "        \n",
    "        Q_actual = self.model.act(state)\n",
    "        Q_sig = self.target.act(next_state)\n",
    "        Q_Esperado = reward + (1-term) * self.gamma * Q_sig\n",
    "\n",
    "        q_loss = F.mse_loss(Q_actual, Q_Esperado.detach())\n",
    "        return q_loss\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Actualización de los parámetros\n",
    "        \"\"\"\n",
    "        loss = self.q_loss()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.requires_grad = True\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Actualización siguiendo Hasselt (2015)\n",
    "        for target_net, net in zip(self.model.parameters(), self.target.parameters()):\n",
    "            target_net.data.copy_(self.tau * net + (1 - self.tau) * target_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d41f5-d459-413d-a167-c44090930d92",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac921ac8-ca7b-4f25-a89c-439ab1d551e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loopPPO(max_steps, PPOAgent, env):\n",
    "    \"\"\"\n",
    "    Genera el proceso de entrenamiento.\n",
    "    \n",
    "    max_steps = int ; Pasos de entrenamiento totales.\n",
    "    PPOAgent = PPO() ; Instancia de PPO por trabajar.\n",
    "    env = gym.env() ; Entorno del gym a trabajar. Por construcción de esta implementación debe de ser uno de estados con representación visual.\n",
    "    Esto es debido a que la red neuronal de nuestra política es convolucional y trabaja sobre los estados visuales del entorno.\n",
    "    \"\"\"\n",
    "    # Hiperparámetros del entrenamiento\n",
    "    max_ep_len = 1000\n",
    "    update_timestep = 5\n",
    "    print_timestep = 20\n",
    "    episode = 0\n",
    "\n",
    "    # Parámetros para imprimir información del entrenamiento\n",
    "    print_running_reward = 0\n",
    "    print_running_episodes = 0\n",
    "    print_freq = 100\n",
    "\n",
    "    best_train_reward = []\n",
    "    best_time_steps = []\n",
    "\n",
    "    reward_records =[] # Para graficar la recompensa\n",
    "    \n",
    "    # Loop de entrenamiento\n",
    "    for episode_count in range(PPOAgent.epochs):\n",
    "        \n",
    "        time_step = 0\n",
    "        epoch_rewards = []\n",
    "        epoch_timesteps = []\n",
    "        \n",
    "        print(f\"Iniciando época: {episode_count+1}\")\n",
    "        \n",
    "        while time_step <= max_steps:\n",
    "            \n",
    "            state, obs = env.reset()\n",
    "            current_ep_reward = 0\n",
    "            \n",
    "            for t in range(1, max_ep_len+1):\n",
    "                # Loop de entrenamiento clásico de Gymnasium\n",
    "                action = PPOAgent.select_action(state)\n",
    "                state, reward, term, trunc, info = env.step(action)\n",
    "    \n",
    "                PPOAgent.buffer.rewards.append(reward)\n",
    "                PPOAgent.buffer.is_terminals.append(term)\n",
    "    \n",
    "                time_step += 1\n",
    "                current_ep_reward += reward\n",
    "                \n",
    "                if t % update_timestep == 0:\n",
    "                    PPOAgent.update()\n",
    "                    epoch_rewards.append(current_ep_reward)\n",
    "                    epoch_timesteps.append(time_step)\n",
    "    \n",
    "                if t % print_freq == 0:\n",
    "                    if print_running_episodes != 0:\n",
    "                        print_avg_reward = print_running_reward / print_running_episodes\n",
    "                        print_avg_reward = round(print_avg_reward, 2)\n",
    "                        print(\"Episodio : {} \\t\\t Timestep : {} \\t\\t Avg Reward : {}\".format(episode+1, time_step, print_avg_reward))\n",
    "                        print_running_reward = 0\n",
    "                        print_running_episodes = 0\n",
    "    \n",
    "                if term:\n",
    "                    print(\"Finished at time step: {}\".format(time_step))\n",
    "                    break\n",
    "\n",
    "    \n",
    "            print_running_reward += current_ep_reward\n",
    "            print_running_episodes += 1\n",
    "\n",
    "        if len(best_train_reward) == 0:\n",
    "            best_train_reward = epoch_rewards\n",
    "\n",
    "        best_train = np.array(best_train_reward)\n",
    "        epoch_r = np.array(epoch_rewards)\n",
    "        print(f\"Premios previos: {best_train.mean()}. Previos actuales: {epoch_r.mean()}\")\n",
    "\n",
    "        if best_train.mean() < epoch_r.mean():\n",
    "            best_train_reward = epoch_rewards\n",
    "            best_time_steps = epoch_timesteps\n",
    "\n",
    "        \n",
    "        reward_records.append(epoch_rewards) # Graficar\n",
    "\n",
    "        episode += 1\n",
    "        print(\"Fin episodio {}\".format(episode))\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "    print(\"Acabamos\")\n",
    "    print(reward_records)\n",
    "    print_plot(reward_records, \"PPO\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f9fc0ba-8456-426d-a999-f197c252d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loopDDQN(update_steps, DDQNAgent, env):\n",
    "    explore_steps = 5000 # Modificable\n",
    "    save_timestep = 5\n",
    "    print_timestep = 20\n",
    "    episode = 0\n",
    "\n",
    "    running_reward = 0\n",
    "    running_eps = 0\n",
    "\n",
    "    best_train_reward = []\n",
    "    \n",
    "    reward_records =[] # Para graficar la recompensa\n",
    "    \n",
    "    for episode_count in range(DDQNAgent.epochs):\n",
    "        time_step = 0\n",
    "        print(f\"Época: {episode_count+1}\")\n",
    "        state, obs = env.reset()\n",
    "        current_ep_reward = 0\n",
    "        epoch_rewards = []\n",
    "        # Primer for (Muestreo)\n",
    "        for t in range(1, explore_steps+1):\n",
    "            action = DDQNAgent.select_action(state)\n",
    "            state, reward, term, trunc, info = env.step(action)\n",
    "            DDQNAgent.buffer.actions.append(action)\n",
    "            DDQNAgent.buffer.states.append(state)\n",
    "            DDQNAgent.buffer.rewards.append(reward)\n",
    "            DDQNAgent.buffer.is_terminal.append(term)\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            if t % save_timestep == 0:\n",
    "                epoch_rewards.append(current_ep_reward)\n",
    "\n",
    "        \n",
    "        reward_arr = np.array(epoch_rewards)\n",
    "        print(f\"Recompensa promedio: {reward_arr.mean()}\")\n",
    "        print(\"--------------------\")\n",
    "\n",
    "        # Segundo for (Update)\n",
    "        for t in range(1, update_steps):\n",
    "            DDQNAgent.update()\n",
    "\n",
    "        # Se resetea el buffer para la siguiente época\n",
    "        DDQNAgent.buffer.clear()\n",
    "        reward_records.append(epoch_rewards)\n",
    "\n",
    "    print(\"Acabamos\")\n",
    "    #print(reward_records)\n",
    "    print_plot(reward_records, \"DDQN\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d4d3ca-6b08-459f-b153-0a19b59e7fcf",
   "metadata": {},
   "source": [
    "### PlayLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b875f660-e505-440f-b944-db25c05a0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_test(Agent, total_episodes, env):\n",
    "    \"\"\"\n",
    "    Hace un loop de prueba de la política aprendida\n",
    "\n",
    "    PPOAgent = PPO() ; Una instancia de PPO ENTRENADA\n",
    "    total_episodes = int ; Cantidad de episodios que se busca probar.\n",
    "    env = gym.env() ; Entorno del gymnasium (Claramente debe de ser el mismo en donde se entrenó PPOAgent())\n",
    "    \"\"\"\n",
    "    max_ep_len = 3000\n",
    "    test_reward = 0\n",
    "    for ep in range(1, total_episodes + 1):\n",
    "        ep_reward = 0\n",
    "        obs, info = env.reset()\n",
    "        for t in range(1, max_ep_len+1):\n",
    "            action = Agent.select_action(obs)\n",
    "            state, reward, term, trunc, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if term or trunc:\n",
    "                break\n",
    "\n",
    "        Agent.buffer.clear()\n",
    "        test_reward += ep_reward\n",
    "    env.close()\n",
    "    print(f\"Recompensa promedio: {round(test_reward/total_episodes, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba34975-4619-4ae9-9ccb-acbfb31584ba",
   "metadata": {},
   "source": [
    "## Ejecuciones completas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e65e0-8104-4dbd-9fb3-2d1b38f1ff3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando época: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at time step: 740\n",
      "Episodio : 1 \t\t Timestep : 840 \t\t Avg Reward : 0.0\n",
      "Finished at time step: 1479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio : 1 \t\t Timestep : 1579 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 1\n",
      "----------------------------\n",
      "Iniciando época: 2\n",
      "Episodio : 2 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 2 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 2 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 2\n",
      "----------------------------\n",
      "Iniciando época: 3\n",
      "Episodio : 3 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 3 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 3 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 3\n",
      "----------------------------\n",
      "Iniciando época: 4\n",
      "Episodio : 4 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 4 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 4 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 4\n",
      "----------------------------\n",
      "Iniciando época: 5\n",
      "Episodio : 5 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 5 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 5 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 5\n",
      "----------------------------\n",
      "Iniciando época: 6\n",
      "Episodio : 6 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 6 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 6 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 6\n",
      "----------------------------\n",
      "Iniciando época: 7\n",
      "Episodio : 7 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 7 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 7 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 7\n",
      "----------------------------\n",
      "Iniciando época: 8\n",
      "Episodio : 8 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 8 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 8 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 8\n",
      "----------------------------\n",
      "Iniciando época: 9\n",
      "Episodio : 9 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 9 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 9 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 9\n",
      "----------------------------\n",
      "Iniciando época: 10\n",
      "Episodio : 10 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 10 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 10 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 10\n",
      "----------------------------\n",
      "Iniciando época: 11\n",
      "Episodio : 11 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n",
      "Episodio : 11 \t\t Timestep : 1100 \t\t Avg Reward : 0.0\n",
      "Episodio : 11 \t\t Timestep : 2100 \t\t Avg Reward : 0.0\n",
      "Premios previos: 0.0. Previos actuales: 0.0\n",
      "Fin episodio 11\n",
      "----------------------------\n",
      "Iniciando época: 12\n",
      "Episodio : 12 \t\t Timestep : 100 \t\t Avg Reward : 0.0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# PPO DonkeyKong\n",
    "\n",
    "epochs = 15\n",
    "clip = 0.175 # El mejor valor empírico del clip termina siendo 0.15\n",
    "gamma = 0.99\n",
    "lr_actor = 0.0003\n",
    "lr_critic = 0.0001\n",
    "\n",
    "PPO_DK = PPO_Self(donkey_kong.observation_space.shape, donkey_kong.action_space.n, lr_actor, lr_critic, gamma, epochs, clip)\n",
    "train_loopPPO(2000, PPO_DK, donkey_kong) # max_steps tiene que ser aprox 3000, menos de eso no aprende, más de eso decae y toma demasiado tiempo\n",
    "play_test(PPO_DK, 10, donkey_kong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e14b11-9307-4197-bcd2-fd9bb1cd7b95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# PPO Breakout\n",
    "\n",
    "PPO_Breakout = PPO_Self(breakout.observation_space.shape, breakout.action_space.n, lr_actor, lr_critic, gamma, epochs, clip)\n",
    "train_loopPPO(1500, PPO_Breakout, breakout) # 1000 max_steps hace que entrene en fa, de 2000 en adelante toma años.\n",
    "play_test(PPO_Breakout, 10, breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a7a73-5200-499b-8f74-30ade6f965c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# DDQN DonkeyKong\n",
    "\n",
    "lr = 0.0003\n",
    "tau = 0.001\n",
    "gamma = 0.99\n",
    "epochs = 25 # Tienen que ser varias épocas para que jale chido\n",
    "\n",
    "DDQN_DonkeyHijoDePerraKong = DDQN_Self(donkey_kong.observation_space.shape, donkey_kong.action_space.n, lr, gamma, tau, epochs)\n",
    "train_loopDDQN(25, DDQN_DonkeyHijoDePerraKong, donkey_kong) \n",
    "play_test(DDQN_DonkeyHijoDePerraKong, 10, donkey_kong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed752c-f1a2-42e6-9ef2-44108edbab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# DDQN Breakout\n",
    "\n",
    "DDQN_Breakout = DDQN_Self(breakout.observation_space.shape, breakout.action_space.n, lr, gamma, tau, 50)\n",
    "train_loopDDQN(30, DDQN_Breakout, breakout)\n",
    "play_test(DDQN_Breakout, 10, breakout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8dcae0-d4f7-4602-818c-4c677a556289",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cdf9d1-f487-469f-9060-80fb9756b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StableBaselines3(algoritmo, env, policy, timesteps):\n",
    "    model = algoritmo(policy, env)\n",
    "    model.learn(total_timesteps = timesteps)\n",
    "    print(\"Done\")\n",
    "    return model\n",
    "\n",
    "def SB3_playloop(model, env, loop_iter):\n",
    "    \"\"\"\n",
    "    model = PPO/DQN ; Modelo de Stabe-Baselines3 entrenado\n",
    "    env = gym.make() ; Entorno del gym.\n",
    "    loop_iter = int ;  Cantidad de veces que probaremos al modelo.\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    avg_reward = 0\n",
    "    for _ in range(loop_iter):\n",
    "        iterations_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = model.predict(obs)\n",
    "            state, reward, term, trunc = env.step(action)\n",
    "            iterations_reward += reward\n",
    "            done = term or trunc\n",
    "        avg_reward += iterations_reward\n",
    "        \n",
    "    print(f\"Ganancia promedio después de {loop_iter} iteraciones: {avg_reward/loop_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ce696-9dce-4dbd-bd7c-05959aa3f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ppo_dk = StableBaselines3(PPO, donkey_kong, \"CnnPolicy\", 200)\n",
    "#dqn_dk = StableBaselines3(DQN, donkey_kong, \"CnnPolicy\", 200)\n",
    "\n",
    "ppo_breakout = StableBaselines3(PPO, breakout, \"CnnPolicy\", 200)\n",
    "#dqn_breakout = StableBaselines3(DQN, breakout, \"CnnPolicy\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aef5bb-e573-4f9b-b4a8-8503dcfdca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "SB3_playloop(ppo_dk, donkey_kong, 10)\n",
    "SB3_playloop(ppo_breakout, breakout, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2656be-69dc-4a9d-bd99-05078154b883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
