{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139c8252-81a5-451c-959c-2af723dfb4fa",
   "metadata": {},
   "source": [
    "# MDPs\n",
    "\n",
    "A continuación se presentan los entornos MDPs (Markov Decision Process) del ajedrez como del juego de gato. El juego de gato está compuesto por dos clases, una del jugador aleatorio y otra del \"tablero\" de gato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3bb19-9fad-4461-8ba2-89fc730f1ffc",
   "metadata": {},
   "source": [
    "## Gato\n",
    "\n",
    "La primera celda contiene el código de la clase de tablero y de jugador aleatorio. Se puede ver un ejemplo de juego entre dos jugadores aleatorios en la segunda celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f0835c-3521-4f42-bbca-5055d37a8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class tictactoe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tablero = np.zeros(9, dtype = int)        \n",
    "\n",
    "    def reset(self):\n",
    "        self.tablero = np.zeros(9, dtype = int)\n",
    "\n",
    "    def valid_space(self, pos):\n",
    "        if self.tablero[pos] == 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def place_move(self, pos, value):\n",
    "        self.tablero[pos] = value\n",
    "\n",
    "    def horizontal_win(self):\n",
    "        for i in range(3):\n",
    "            if self.tablero[0+(3*i)] == self.tablero[1+(3*i)] == self.tablero[2+(3*i)] and self.tablero[0+(3*i)] != 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def vertical_win(self):\n",
    "        for i in range(3):\n",
    "            if self.tablero[i] == self.tablero[i+3] == self.tablero[i+6] and self.tablero[i] != 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def diagonal_win(self):\n",
    "        diag1 = (self.tablero[0] == self.tablero[4] == self.tablero[8] and self.tablero[0] != 0)\n",
    "        diag2 = (self.tablero[2] == self.tablero[4] == self.tablero[6] and self.tablero[2] != 0)\n",
    "        if diag1 or diag2:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_win(self):\n",
    "        if self.horizontal_win() or self.vertical_win() or self.diagonal_win():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_space(self):\n",
    "        for _ in self.tablero:\n",
    "            if _ == 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def step(self, player):\n",
    "        player.play_tile(self)\n",
    "        if self.check_win():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "class random_player:\n",
    "    \n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.reward = 0\n",
    "\n",
    "    def play_tile(self, ttt):\n",
    "        \"\"\"\n",
    "        Realiza un tiro aleatorio, siempre y cuando la posición sea válida.\n",
    "        \"\"\"\n",
    "        if ttt.is_space():\n",
    "            while True:\n",
    "                random = np.random.randint(9)\n",
    "                if ttt.valid_space(random):\n",
    "                    ttt.place_move(random, self.value)\n",
    "                    break\n",
    "\n",
    "\n",
    "def play_loop(x, o, tic, iteraciones):\n",
    "    \"\"\"\n",
    "    Loop del juego. Se detiene cuando alguno de los dos jugadores gana, o cuando ya no hay espacio disponible.\n",
    "\n",
    "    x, o = player() ; instancias de la clase de jugadores\n",
    "    tic = tictactoe() ; instancia de la clase de tableros.\n",
    "    \"\"\"\n",
    "    x_score = 0\n",
    "    o_score = 0\n",
    "    for i in range(iteraciones):\n",
    "        tic.reset()\n",
    "        print(f'Juego {i+1}')\n",
    "        while(True):\n",
    "            rewardx = tic.step(x)\n",
    "            if tic.check_win():\n",
    "                print('Gana X')\n",
    "                x.reward = rewardx\n",
    "                o.reward = -1\n",
    "                break\n",
    "            rewardy = tic.step(o)\n",
    "            if tic.check_win():\n",
    "                print('Gana O')\n",
    "                o.reward = rewardy\n",
    "                x.reward = -1\n",
    "                break\n",
    "            if not tic.is_space():\n",
    "                print('Gato')\n",
    "                x.reward = o.reward = 0\n",
    "                break\n",
    "        x_score += x.reward # Dependiendo de lo que necesitemos se puede modificar este valor para tener una recompensa total.\n",
    "        o_score += o.reward\n",
    "        print(tic.tablero[:3])\n",
    "        print(tic.tablero[3:6])\n",
    "        print(tic.tablero[6:])\n",
    "        print('------')\n",
    "    print(f'Puntuaciones finales: X = {x_score}. O = {o_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdf0ad5-ab9c-4de6-bbca-525af18b4c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juego 1\n",
      "Gana O\n",
      "[2 2 2]\n",
      "[2 1 1]\n",
      "[1 1 0]\n",
      "------\n",
      "Juego 2\n",
      "Gana O\n",
      "[1 1 0]\n",
      "[2 2 2]\n",
      "[0 1 0]\n",
      "------\n",
      "Juego 3\n",
      "Gana X\n",
      "[2 2 1]\n",
      "[1 2 2]\n",
      "[1 1 1]\n",
      "------\n",
      "Juego 4\n",
      "Gato\n",
      "[1 1 2]\n",
      "[2 1 1]\n",
      "[1 2 2]\n",
      "------\n",
      "Juego 5\n",
      "Gana O\n",
      "[0 2 0]\n",
      "[1 2 0]\n",
      "[1 2 1]\n",
      "------\n",
      "Juego 6\n",
      "Gana X\n",
      "[2 2 1]\n",
      "[1 1 0]\n",
      "[1 0 2]\n",
      "------\n",
      "Juego 7\n",
      "Gana O\n",
      "[1 0 2]\n",
      "[1 2 2]\n",
      "[2 1 1]\n",
      "------\n",
      "Juego 8\n",
      "Gana X\n",
      "[2 1 2]\n",
      "[1 1 1]\n",
      "[1 2 2]\n",
      "------\n",
      "Juego 9\n",
      "Gato\n",
      "[1 2 1]\n",
      "[1 1 2]\n",
      "[2 1 2]\n",
      "------\n",
      "Juego 10\n",
      "Gana X\n",
      "[1 1 1]\n",
      "[2 0 2]\n",
      "[2 0 1]\n",
      "------\n",
      "Puntuaciones finales: X = 0. O = 0\n"
     ]
    }
   ],
   "source": [
    "a = tictactoe()\n",
    "xp = random_player(1)\n",
    "op = random_player(2)\n",
    "play_loop(xp, op, a, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97307436-f462-488c-85a1-bfff128dd248",
   "metadata": {},
   "source": [
    "## FrozenLake\n",
    "\n",
    "Al ser un entorno del gymnasium podemos acceder fácilmente a las distintas propiedades del entorno sin tener que programar tanto como en el caso del gato. Se presentan dos formas de calcular las funciones de valor y las funciones de estado, ambas siguen el mismo formato del algoritmo visto dentro del libro, no obstante una se presenta dentro de la clase de un agente, mientras que otra se presenta como función aislada. (Algo de la implementación interna del agente no funciona bien, nos da resultados distintos a lo de la implementación externa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae63f496-d3ec-49c7-8906-753b20349e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f2a0713-f99b-4aba-97c7-d2170f0dbc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_env = gym.make(\"FrozenLake-v1\", render_mode = \"human\", is_slippery = True)\n",
    "fl_env = fl_env.unwrapped\n",
    "obs, info = fl_env.reset()\n",
    "\n",
    "fl_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1eb38e4-afd2-4743-a64c-bc9793ca60cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)]},\n",
       " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La dinámica del ambiente está dado por\n",
    "fl_env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6c185b-b3a8-401a-9fbe-b7d449997386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Elf:\n",
    "    def __init__(self):\n",
    "        self.env = fl_env\n",
    "        self.state_val = np.zeros(self.env.observation_space.n)\n",
    "        self.policy = np.zeros(self.env.observation_space.n)\n",
    "        self.gamma = 0.95\n",
    "        self.delta = 0.003\n",
    "\n",
    "    def state_value(self):\n",
    "        \"\"\"\n",
    "        Evaluación de la política\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            update = np.copy(self.state_val)\n",
    "            for s in range(self.env.observation_space.n):\n",
    "                action = self.policy[s]\n",
    "                self.state_val[s] = sum([t_prob * (r_prob + self.gamma * update[next_s])\n",
    "                                          for t_prob, next_s, r_prob, _ in fl_env.P[s][action]])\n",
    "\n",
    "            if (np.sum(np.fabs(update-self.state_val)) <= self.delta):\n",
    "                break\n",
    "        return self.state_val\n",
    "\n",
    "    def new_pol(self):\n",
    "        \"\"\"\n",
    "        Mejora de política\n",
    "        \"\"\"\n",
    "        for s in range(self.env.observation_space.n):\n",
    "            q = np.zeros(self.env.observation_space.n)\n",
    "            for action in range(self.env.action_space.n):\n",
    "                for next_sr in self.env.P[s][action]:\n",
    "                    t_prob, next_s, r_prob, _ = next_sr\n",
    "                    q[action] += (t_prob * (r_prob + self.gamma * self.state_val[next_s]))\n",
    "                    \n",
    "            self.policy[s] = np.argmax(q)\n",
    "        return self.policy \n",
    "\n",
    "    def policy_iteration(self, eps):\n",
    "        \"\"\"\n",
    "        Algoritmo completo\n",
    "        \"\"\"\n",
    "        policy = self.policy\n",
    "        for i in range(eps):\n",
    "            f_value = self.state_value()\n",
    "            self.policy = self.new_pol()\n",
    "\n",
    "            if (np.all(policy == self.policy)):\n",
    "                break\n",
    "            policy = self.policy\n",
    "        return self.policy, f_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02586c54-8df3-4271-9891-f2e2af8657d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "elfo = Elf()\n",
    "\n",
    "elf_pol, elf_value = elfo.policy_iteration(100)\n",
    "print(elf_pol, elf_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb04c1e6-5a26-40ac-9178-311b70c03a28",
   "metadata": {},
   "source": [
    "#### Iteración de Política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7843e20-54b4-4cd9-8400-9c030d478483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versión fuera de la clase del agente.\n",
    "def state_value(pi, gamma):\n",
    "    \"\"\"\n",
    "    Evaluación de la política\n",
    "    \"\"\"\n",
    "    value_function = np.zeros(fl_env.observation_space.n)\n",
    "    delta = 0.003\n",
    "\n",
    "    while True:\n",
    "        update = np.copy(value_function)\n",
    "        for state in range(fl_env.observation_space.n):\n",
    "            action = pi[state]\n",
    "            value_function[state] = sum([t_prob * (r_prob + gamma * update[next_state])\n",
    "                                        for t_prob, next_state, r_prob, _ in fl_env.P[state][action]])\n",
    "\n",
    "        if (np.sum(np.fabs(update - value_function)) <= delta):\n",
    "            #print(\"Convergencia\")\n",
    "            break\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def nueva_pi(value_function, gamma):\n",
    "    \"\"\"\n",
    "    Mejora de política\n",
    "    \"\"\"\n",
    "    pi = np.zeros(fl_env.observation_space.n)\n",
    "\n",
    "    for state in range(fl_env.observation_space.n):\n",
    "        q = np.zeros(fl_env.action_space.n)\n",
    "        for action in range(fl_env.action_space.n):\n",
    "            for next_sr in fl_env.P[state][action]:\n",
    "                t_prob, next_state, r_prob, _ = next_sr\n",
    "                q[action] += (t_prob * (r_prob + gamma * value_function[next_state]))\n",
    "\n",
    "        pi[state] = np.argmax(q)\n",
    "    return pi\n",
    "\n",
    "\n",
    "def policy_iteration(env, gamma):\n",
    "    \"\"\"\n",
    "    Algoritmo completo\n",
    "    \"\"\"\n",
    "    pi = np.zeros(env.observation_space.n)\n",
    "    episodios = 100\n",
    "\n",
    "    for i in range(episodios):\n",
    "        valor_f = state_value(pi, gamma)\n",
    "        new_pi = nueva_pi(valor_f, gamma)\n",
    "\n",
    "        if (np.all(pi == new_pi)):\n",
    "            break\n",
    "        pi = new_pi\n",
    "    return new_pi, valor_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "913257dc-030a-42d2-a6a9-1de400f0852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política: [0. 3. 0. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]. \n",
      " La función de valor: [0.17546028 0.15051032 0.15038052 0.12914343 0.20441015 0.\n",
      " 0.17476461 0.         0.26676807 0.37216449 0.40192189 0.\n",
      " 0.         0.50718474 0.72273331 0.        ]\n"
     ]
    }
   ],
   "source": [
    "policy, value = policy_iteration(fl_env, 0.95)\n",
    "print(f\"Política: {policy}. \\n La función de valor: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f6740-5915-4e9b-a7db-5f13e50392c1",
   "metadata": {},
   "source": [
    "#### Iteración de Valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43699a6c-8883-48eb-a0b4-ad97748814ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 0.95):\n",
    "    \"\"\"\n",
    "    Algoritmo completo\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicio\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    episodios = 100\n",
    "    umbral = 0.00005\n",
    "    \n",
    "    for i in range(episodios):\n",
    "        updated_value_table = np.copy(value_table) \n",
    "        \n",
    "        # Cálculo de Q y reajuste máximo        \n",
    "        for state in range(env.observation_space.n):\n",
    "            Q_value = []\n",
    "            for action in range(env.action_space.n):\n",
    "                next_states_rewards = []\n",
    "                for next_sr in env.P[state][action]: \n",
    "                    trans_prob, next_state, reward_prob, _ = next_sr \n",
    "                    next_states_rewards.append((trans_prob * (reward_prob + gamma * updated_value_table[next_state]))) \n",
    "                \n",
    "                Q_value.append(np.sum(next_states_rewards))\n",
    "                \n",
    "            value_table[state] = max(Q_value) \n",
    "            \n",
    "        if (np.sum(np.fabs(updated_value_table - value_table)) <= umbral):\n",
    "             #print(f'¡Convergencia! Iteración: {i+1}')\n",
    "             break\n",
    "    \n",
    "    return value_table\n",
    "\n",
    "\n",
    "def extract_policy(value_table, gamma = 1.0):\n",
    "    \"\"\"\n",
    "    Obtención de política\n",
    "    \"\"\"\n",
    "    policy = np.zeros(fl_env.observation_space.n) \n",
    "    for state in range(fl_env.observation_space.n):\n",
    "        \n",
    "        # Inicio de Q en cero\n",
    "        Q_table = np.zeros(fl_env.action_space.n)\n",
    "        \n",
    "        # Cálculo de Q\n",
    "        for action in range(fl_env.action_space.n):\n",
    "            for next_sr in fl_env.P[state][action]: \n",
    "                trans_prob, next_state, reward_prob, _ = next_sr \n",
    "                Q_table[action] += (trans_prob * (reward_prob + gamma * value_table[next_state]))\n",
    "\n",
    "        policy[state] = np.argmax(Q_table)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2f0f258-df56-4fcf-97e4-6a872527536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política: [0. 3. 0. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]. \n",
      " Value_f: [0.18035745 0.15466    0.15340661 0.13247087 0.20886331 0.\n",
      " 0.17639284 0.         0.27037339 0.37459489 0.40363285 0.\n",
      " 0.         0.50893908 0.72365223 0.        ]\n"
     ]
    }
   ],
   "source": [
    "opt_val = value_iteration(fl_env, 0.95)\n",
    "policy = extract_policy(opt_val, 0.95)\n",
    "print(f\"Política: {policy}. \\n Value_f: {opt_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e1be7-d457-4b8f-8831-e48e7a02372e",
   "metadata": {},
   "source": [
    "## Comentarios:\n",
    "\n",
    "Los algoritmos con la implementación del gym son bastante sencillos para implementar ya que el gym mismo te regresa todos los valores necesarios, dígase estados siguientes, recompensa etc. No obstante no logré encontrar una forma de representar el gato de tal forma que me regresara los valores de la misma manera, una opción que exploré (un poco tarde) fue representar el gato como entorno del gymnaisum mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f41af7-0f8e-45af-aaac-a0b1079a694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [1 1 0]\n",
      " [0 1 1]] 7\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "space = gym.spaces.Box(low = 0, high = 2, shape = (3,3), dtype = int)\n",
    "acciones = gym.spaces.Discrete(9)\n",
    "aux = space.sample()\n",
    "act = acciones.sample()\n",
    "print(aux, act)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70942f2d-4e35-4d37-a108-554153ecc03b",
   "metadata": {},
   "source": [
    "**¿Ese acercamiento es más factible que la representación desde cero?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
