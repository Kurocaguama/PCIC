{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea044bb-85f7-4792-87c0-7aaba13bee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elaleph.txt', 'elevangeliosegunmarcos.txt', 'cartaaunasenoritaenparis.txt', 'casatomada.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\FLopezP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "from nltk.tokenize import PunktTokenizer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "sentence_tokenizer = PunktTokenizer()\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def get_file(text):\n",
    "    with open(r'C:\\Users\\FLopezP\\Documents\\GitHub\\PCIC\\PLN\\Documentos\\{}'.format(text), encoding = 'utf8') as f:\n",
    "        aux = f.read()\n",
    "    return aux\n",
    "\n",
    "#Cargar datos\n",
    "files = [\"elaleph.txt\",\"elevangeliosegunmarcos.txt\",\"cartaaunasenoritaenparis.txt\",\"casatomada.txt\"]\n",
    "textos = []\n",
    "textos = [get_file(_) for _ in files]\n",
    "all_text = ' '.join(textos)\n",
    "print(files)\n",
    "\n",
    "def PredictAuthors(fvs):\n",
    "    '''\n",
    "    Se utiliza el algoritmo de K-Medias como método de clusterización (No supervisado)\n",
    "    '''\n",
    "    km = KMeans(n_clusters=2, init='k-means++', n_init=20, verbose=0)\n",
    "    km.fit(fvs)\n",
    "    return km\n",
    "    \n",
    "def Features():\n",
    "    '''\n",
    "    Obtener vectores de características, utilizando características estilométricas de caracteres, léxicas y sintácticas.\n",
    "    '''\n",
    "    num_textos = len(textos)\n",
    "    fvs = np.zeros((len(textos), 9), np.float64)\n",
    "    for e, ch_text in enumerate(textos):\n",
    "        contMayus = 0\n",
    "        contMinus = 0\n",
    "        contNumeros = 0\n",
    "        textoNormal = ch_text\n",
    "        textoMinusculas = ch_text.lower()\n",
    "        charTotales = len(textoNormal)\n",
    "        texto = ch_text.lower()\n",
    "        tokens = nltk.word_tokenize(texto)\n",
    "        words = word_tokenizer.tokenize(texto)\n",
    "        sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                       for s in sentences])\n",
    "        for letra in textoNormal:\n",
    "            if letra.isupper():\n",
    "                contMayus = contMayus + 1\n",
    "            else:\n",
    "                contMinus = contMinus + 1\n",
    "            if letra in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]:\n",
    "                contNumeros = contNumeros + 1\n",
    "        # Número promedio de palabras por oración\n",
    "        fvs[e, 0] = words_per_sentence.mean()\n",
    "        # Variación del tamaño de las oraciones\n",
    "        fvs[e, 1] = words_per_sentence.std()\n",
    "        # Diversidad léxica\n",
    "        fvs[e, 2] = len(vocab) / float(len(words))\n",
    "        # Número de comas por oración\n",
    "        fvs[e, 3] = tokens.count(',') / float(len(sentences))\n",
    "        # Número de puntos y comas por oración\n",
    "        fvs[e, 4] = tokens.count(';') / float(len(sentences))\n",
    "        # Número de dos puntos por oración\n",
    "        fvs[e, 5] = tokens.count(':') / float(len(sentences))\n",
    "        # Proporción mayúsculas/total de caracteres\n",
    "        fvs[e, 6] = float(contMayus/charTotales)\n",
    "\t\t# Proporción minúsculas/total de caracteres\n",
    "        fvs[e, 7] = float(contMinus/charTotales)\n",
    "        # Proporción de números respecto a letras\n",
    "        fvs[e, 8] = float(contNumeros/charTotales)\n",
    "\n",
    "    fvs = whiten(fvs)\n",
    "    return fvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f863f53-b279-4549-8afc-92b74c911085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "classifications = []\n",
    "feature_sets = []\n",
    "for element in list(Features()):\n",
    "    feature_sets.append(element)\n",
    "feature_sets = np.array(feature_sets)\n",
    "result = PredictAuthors(feature_sets).labels_\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209d043-8c91-42da-864f-d493426a2f50",
   "metadata": {},
   "source": [
    "Podemos notar que la clasificación es errónea ya que son 3 clasificados como textos de Cortazar, y solo uno de Borges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7932e718-77c4-4ceb-80b0-29cfd4fa2fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def Features_Nueva():\n",
    "    '''\n",
    "    Obtener vectores de características, utilizando características estilométricas de caracteres, léxicas y sintácticas.\n",
    "    '''\n",
    "    num_textos = len(textos)\n",
    "    fvs = np.zeros((len(textos), 9), np.float64)\n",
    "    for e, ch_text in enumerate(textos):\n",
    "        contMayus = 0\n",
    "        contMinus = 0\n",
    "        contNumeros = 0\n",
    "        textoNormal = ch_text\n",
    "        textoMinusculas = ch_text.lower()\n",
    "        charTotales = len(textoNormal)\n",
    "        texto = ch_text.lower()\n",
    "        tokens = nltk.word_tokenize(texto)\n",
    "        words = word_tokenizer.tokenize(texto)\n",
    "        sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                       for s in sentences])\n",
    "        for letra in textoNormal:\n",
    "            if letra.isupper():\n",
    "                contMayus = contMayus + 1\n",
    "            else:\n",
    "                contMinus = contMinus + 1\n",
    "            if letra in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]:\n",
    "                contNumeros = contNumeros + 1\n",
    "        # Número promedio de palabras por oración\n",
    "        fvs[e, 0] = words_per_sentence.mean()\n",
    "        # Variación del tamaño de las oraciones\n",
    "        fvs[e, 1] = words_per_sentence.std()\n",
    "        # Diversidad léxica\n",
    "        fvs[e, 2] = len(vocab) / float(len(words))\n",
    "        # Número de comas por oración\n",
    "        fvs[e, 3] = tokens.count(',') / float(len(sentences))        \n",
    "        # Número de dos puntos por oración\n",
    "        fvs[e, 5] = tokens.count(':') / float(len(sentences))\n",
    "\t\t# Proporción minúsculas/total de caracteres\n",
    "        fvs[e, 7] = float(contMinus/charTotales)\n",
    "        # Proporción de números respecto a letras\n",
    "        fvs[e, 8] = float(contNumeros/charTotales)\n",
    "        \n",
    "        # CARACTERÍSTICAS EXTRA\n",
    "        fvs[e, 4] = tokens.count('.') / float(len(sentences))\n",
    "        fvs[e, 6] = len(vocab)\n",
    "\n",
    "    fvs = whiten(fvs)\n",
    "    return fvs\n",
    "\n",
    "classifications = []\n",
    "feature_sets = []\n",
    "for element in list(Features_Nueva()):\n",
    "    feature_sets.append(element)\n",
    "feature_sets = np.array(feature_sets)\n",
    "result = PredictAuthors(feature_sets).labels_\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f4632-d92c-46cf-b9fc-a3571cde224e",
   "metadata": {},
   "source": [
    "Cambié ; por ., y puse la longitud del vocabulario como característica, pero siguen dando los mismos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03993df8-27e3-4cdf-9b86-c41e615c5877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
